name: Runtimes

on:
  workflow_dispatch:

jobs:
  build-upload:
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@v4
      - name: "Setup yq"
        uses: dcarbone/install-yq-action@v1.1.1
      - name: "Configure AWS credentials"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          mask-aws-account-id: true
      - name: "Build runtimes and upload"
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          parameters_json=$(yq -o=json '.' parameters.yml)
          bucket_name=$(jq -r '.bucket_name' <<< $parameters_json)
          
          bucket_status=$(aws s3api head-bucket --bucket $bucket_name --region $AWS_REGION 2>&1) || true

          if echo $bucket_status | grep -q 'Not Found'; then
            echo "Bucket '$bucket_name' does not exist. Creating..."
            aws s3api create-bucket \
              --bucket $bucket_name \
              --region $AWS_REGION \
              --create-bucket-configuration LocationConstraint=$AWS_REGION
          else
            echo "Bucket $bucket_name exists"
          fi

          if ! aws s3api head-bucket --bucket $bucket_name 2>/dev/null; then
            aws s3api create-bucket --bucket $bucket_name --region $AWS_REGION \
            --create-bucket-configuration LocationConstraint=$AWS_REGION
          fi

          file_paths=$(find runtimes -type f -name build.sh -exec dirname {} \;)

          for file_path in $file_paths
          do
            echo "::group::Process '$file_path'"

            manifest_json=$(yq -o=json '.' "$file_path/manifest.yml" -j)

            architectures=$(jq -r '.architectures[]' <<< "$manifest_json")
            memory_sizes=$(jq -r '.memory_sizes[]' <<< "$parameters_json")
            path=$(jq -r '.path' <<< "$manifest_json")

            chmod +x "$file_path/build.sh"

            for arch in $architectures
            do
              echo "::group::Build '$file_path' [$arch]"
              (cd "$file_path" && ./build.sh "$arch")
              echo "Building '$file_path' [$arch] completed"
              echo "::endgroup::"

              zip="${path}_${arch}.zip"
              file="${file_path}/${zip}"
              s3_key="runtimes/${zip}"

              if [ -f "$file" ]; then
                echo "::group::Upload '$file_path' [$arch]"
                aws s3 cp "$file" s3://$bucket_name/runtimes/ > /dev/null 2>&1
                echo "Uploaded '$file' to 's3://$bucket_name/$s3_key'"
          
                for memory_size in $memory_sizes
                do
                  function_name="lambda-benchmark-${path}-${arch}-${memory_size}"
          
                  if aws lambda get-function --function-name "$function_name" > /dev/null 2>&1; then
                    aws lambda update-function-code --function-name "$function_name" --s3-bucket "$bucket_name" --s3-key "$s3_key" > /dev/null
                    echo "Function '$function_name' code updated"
                  fi
                done
                echo "::endgroup::"
              else
                echo "File '$file' does not exist. Skipping upload."
              fi
            done
            echo "::endgroup::"
          done
